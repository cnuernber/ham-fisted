<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><title>Reductions</title><script async="true" src="https://www.googletagmanager.com/gtag/js?id=G-XJYNJF48RM"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-XJYNJF48RM');</script><link rel="stylesheet" type="text/css" href="css/default.css" /><link rel="stylesheet" type="text/css" href="highlight/solarized-light.css" /><script type="text/javascript" src="highlight/highlight.min.js"></script><script type="text/javascript" src="js/jquery.min.js"></script><script type="text/javascript" src="js/page_effects.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a> with <a href="https://github.com/xsc/codox-theme-rdash">RDash UI</a> theme</h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Ham-Fisted</span> <span class="project-version">1.000-beta-40</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Topics</span></h3><ul><li class="depth-1  current"><a href="Reductions.html"><div class="inner"><span>Reductions</span></div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>ham-fisted</span></div></div></li><li class="depth-2 branch"><a href="ham-fisted.api.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>api</span></div></a></li><li class="depth-2 branch"><a href="ham-fisted.lazy-noncaching.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>lazy-noncaching</span></div></a></li><li class="depth-2"><a href="ham-fisted.protocols.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>protocols</span></div></a></li></ul></div><div class="document" id="content"><div class="doc"><div class="markdown"><h1>Reductions</h1>
<p>In ham-fisted I have attempted to extend the concept of reduction in Clojure a few
ways taking influence from java streams and Clojure transducers.  The most important
way is a formal definition of a parallel reduction.</p>
<p>We are only interested in the 4 argument form of reductions <code>(reduce rfn init coll)</code>.
It turns out there are type issues with the 3 argument form <code>(reduce rfn coll)</code> as the
reduction function - <code>rfn</code>'s leftmost argument could either be a value
from the collection <em>or</em> an accumulated value.  While there are reductions where the
accumulator is in the set of objects in the collection such as numeric <code>+</code>, these are
not the most general and are a special case of a reduction where the accumulator may
be a different type entirely than the values in the collection.</p>
<h2>Parallelizable Containers</h2>
<p>In Java there are three types of containers that we can efficiently parallelize.
The first two are finite contains that are either randomly addressable or have a
spliterator design where we can divide the elements roughly but perhaps not
precisely in half.  A array is an example of a randomly addressable container
while a hashtable is an example of a container that allows us to split our iterator
ending up with 2 iterators and dividing the elements roughly in half.</p>
<p>The third type of container is a concatenation many other containers
where we can parallelize reduction across each sub-container in one of two ways, either
by in parallel reducing each sub-container or by performing a parallelized reduction
across each sub-container container.</p>
<p>Regardless, we end up with a few types of containers we can parallelize; random access
containers, maps and sets or more generally anything with a correct spliterator
implementation, and concatenations of sub-containers each of which may not itself
have a parallelizable reduction.</p>
<h2>Parallelized Reduction</h2>
<p>A parallelized reduction must generally have a way of splitting up elements of the data
source.  Then it will create many reduction contexts each of which will perform a
serial reduction.  Finally it will need to merge the results back together.  I think this
is perhaps a clearer definition of map-reduce than map-reduce but either way I think it
is useful to compare a parallelized reduction in detail to a serial reduction.</p>
<p>So our parallelizable reduction entry point must have at least four things from the user:</p>
<ul>
<li><code>init-val-fn</code> - A function to produce initial accumulators for each reduction context.</li>
<li><code>rfn</code> - a function that takes an accumulator and a value and updates the accumulator.  This
is the typical reduction operator used for clojure's <code>reduce</code> function.</li>
<li><code>merge-fn</code> - a function that takes two accumulators and merges them to produces one
result accumulator.</li>
</ul>
<p>Here are the function signatures:</p>
<pre><code class="language-clojure">(defn preduce [init-val-fn rfn merge-fn coll] ...)
</code></pre>
<p>We should note that the java stream 'collect' method takes the same four arguments where
the collection is the <code>this</code> object:</p>
<pre><code class="language-java">interface Stream&lt;E&gt; {
&lt;R&gt; R collect(Supplier&lt;R&gt; supplier,
              BiConsumer&lt;R,? super T&gt; accumulator,
              BiConsumer&lt;R,R&gt; combiner);
}
</code></pre>
<p>The parallelizable reduction breaks down in a serial reduction when the init-val-fn is called
once with no arguments and the entire collection is passed along with rfn to reduce:</p>
<pre><code class="language-clojure">(reduce rfn (init-val-fn) coll)
</code></pre>
<p>From there we can imagine <code>preduce</code> switching on the type of coll and performing one of four
distinct types of reductions:</p>
<ul>
<li>serial</li>
<li>parallelizing over and index space.</li>
<li>parallelizing over and spliterator space.</li>
<li>parallelizing over sub-containers.</li>
</ul>
<h2>Map, Filter, Concat Chains</h2>
<p>It is common in functional programming or perhaps ubiquitous to implement your data
transformations as chains of map, filter, and concat operations.  Analyzing sequences
of these operations yields a few insights with regards to reduction in general
and parallelization of reductions.</p>
<p>The first insight is found in the Clojure transducer pathways and involves collapsing
the reduction function when possible for map and filter applications.  Let's start with
a reduction of the form <code>(-&gt;&gt; coll (map x) (filter y) (reduce ...))</code>.</p>
<p>The filter operator can specialize its reduce implementation by producing a new reduction
function and reducing over its source collection:</p>
<pre><code class="language-java">public Object reduce(IFn rfn, Object init) {
	return source.reduce(new IFn() {
	  public Object invoke(Object acc, Object v) {
	    if(pred.test(v))
		  return rfn.invoke(acc, v);
	    else
		  return acc;
	  }
	}, init);
}
</code></pre>
<p>This results in 'collapsing' the reduction allowing the source to perform the
iteration across its elements and simply dynamically creating a slightly more
complex reduction function, <code>rfn</code>.  A similar pathway exists for map as we can always delegate
up the chain making a slightly more complex reduction function as long as we are reducing
over a single source of data.  This same optimization is done automatically Clojure's
transducer implementations.</p>
<p>Collapsing the reduction also allows us to parallelize reductions like the initial one
stated before as if the filter object has a parallelReduction method that does an identical
collapsing pathway then if the source is parallelizable then the reduction itself can
still parallelize:</p>
<pre><code class="language-java">public Object parallelReduction(IFn initValFn, IFn rfn, IFn mergeFn) {
  return source.parallelReduction(initValFn, new IFn() {...}, mergeFn);
}
</code></pre>
<p>In this way we can parallelize reductions over map,filter chains assuming the source data
itself allows for a parallelized reduction.  This optimization is <em>not</em> done in
transducer pathways but it <em>is</em> done with java streams.</p>
<p>These optimizations are only available if we use the 4 argument form of reduce <em>and</em> if
we assume that map, filter, and concat are lazy and non-caching.</p>
<p>Given those assumtions, however, this means that we can parallelize a reduction over the
entries, keys or values of map using simple primitive composition:</p>
<pre><code class="language-clojure">user&gt; (require '[ham-fisted.api :as hamf])
nil
user&gt; (require '[ham-fisted.lazy-noncaching :as lznc])
nil
user&gt; (def data (hamf/immut-map (lznc/map #(vector % %) (range 20000))))
#'user/data
user&gt; (type data)
ham_fisted.PersistentHashMap
user&gt; (hamf/preduce + + + (lznc/map key data))
199990000
</code></pre>
<h2>Stream-based Reductions</h2>
<p>Java streams have an interesting parallelization design that currently suffers from
two flaws, one minor and one major.</p>
<p>The first minor is that you can ask a stream for a parallel version of itself and it will
give you one if possible else return a copy of itself.  Unfortunately this only
works on the first stream in a pipeline so for instance:</p>
<pre><code class="language-java">  coll.stream().map().filter().parallel().collect();
</code></pre>
<p>yeilds a serial reduction while:</p>
<pre><code class="language-java">  coll.stream().parallel().map().filter().collect();
</code></pre>
<p>yeilds a parallel reduction.</p>
<p>This is unfortunate because it means you must go back in time
to get a parallel version of the stream if you want to perform a parallel collection;
something that may or may not be easily done at the point in time when you decide you do
in fact want to parallel reduction.</p>
<p>The second more major flaw is stream-based parallelization is hampered additionally in that
it does not allow the user to pass in a fork-join pool at any point and thus it only works
on cpu-based reductions that can never hang in the ForkJoinPool's common pool.</p>
<h2>reducers.clj And Parallel Folds</h2>
<p>Clojure has an alpha namespace that provides a parallel reduction, <a href="https://github.com/clojure/clojure/blob/master/src/clj/clojure/core/reducers.clj">reducers.clj</a>.  The signature
for this method is:</p>
<pre><code class="language-clojure">(defn fold
  "Reduces a collection using a (potentially parallel) reduce-combine
  strategy. The collection is partitioned into groups of approximately
  n (default 512), each of which is reduced with reducef (with a seed
  value obtained by calling (combinef) with no arguments). The results
  of these reductions are then reduced with combinef (default
  reducef). combinef must be associative, and, when called with no
  arguments, (combinef) must produce its identity element. These
  operations may be performed in parallel, but the results will
  preserve order."
  {:added "1.5"}
  ([reducef coll] (fold reducef reducef coll))
  ([combinef reducef coll] (fold 512 combinef reducef coll))
  ([n combinef reducef coll]
     (coll-fold coll n combinef reducef)))
</code></pre>
<p>In this case we use overloads of <code>combinef</code> or <code>reducef</code> to provider the initial accumulator
(called the identity element), the rfn, finalization and the merge function.  <code>combinef</code> called
with no arguments provides each thread context's accumulator and called with two arguments
performs a merge of two accumulators.  <code>reducef</code> called with 2 arguments provides
the reduction from a value into the accumulator and when called with one argument
finalizes both the potentially stateful reducing function and finalizes the
accumulator.  It prescribes the parallelization system but users can override a protocol
to do it themselves.</p>
<p>This the same major drawback as the java stream system, namely users cannot provide
their own pool for parallelization.</p>
<p>An interesting decision was made here as to whether one can actually parallelize the
reduction or not.  Transducers, the elements providing <code>reducef</code>, may be stateful
such as <code>(take 15)</code>.  One interesting difference is that state is done with a closure in
the reduction function as opposed to providing a custom accumulator that wraps the user's
accumulator but tracks state.</p>
<p>One aspect we haven't discussed but that is also handled here in an interesting manner
is that whether a reduction can be parallelized or not is a function both of the container
<em>and</em> of the reducer.  <code>reducers.clj</code> does a sort of double-dispatch where the transducer
may choose to implement the parallel reduction, called <code>coll-fold</code> or not and is queried
first and if it allows parallel reduction then the collection itself is dispatched upon.
Overall this is a great, safe choice because it disallows completely parallel dispatch if the
transducer or the collection do not support it.</p>
<h2>Parallel Reducers</h2>
<p>If we combine all three functions: <code>init-val-fn</code>, <code>rfn</code>, and <code>merge-fn</code> into one object
then we get a ParallelReducer, defined in protocols.clj.  This protocol allows the
user to pass a single object into a parallelized reduction as opposed to three functions
which is useful when we want to have many reducers reduce over a single source of data.
A <code>finalize</code> method is added in order to allow compositions of reducers and to allow
reducers to elide state and information from end users:</p>
<pre><code class="language-clojure">(defprotocol ParallelReducer
  "Parallel reducers are simple a single object that you can pass into preduce as
  opposed to 3 separate functions."
  (-&gt;init-val-fn [item]
    "Returns the initial values for a parallel reduction.  This function
takes no arguments and returns the initial reduction value.")
  (-&gt;rfn [item]
    "Returns the reduction function for a parallel reduction. This function takes
two arguments, the initial value and a value from the collection and returns a new
initial value.")
  (-&gt;merge-fn [item]
    "Returns the merge function for a parallel reduction.  This function takes
two initial values and returns a new initial value.")
  (finalize [item v]
    "A finalize function called on the result of the reduction after it is
reduced but before it is returned to the user.  Identity is a reasonable default."))
</code></pre>
<p>There are defaults to the reducer protocol for an IFn which simple assumes it can be
called with no arguments for a initial value and two arguments for both reduction
and merge.  This works for things like <code>+</code> and <code>*</code>.  Additionally there are implementations
provided for the ham_fisted Sum (Kahans compensated) and SimpleSum <a href="https://docs.oracle.com/javase/8/docs/api/java/util/function/DoubleConsumer.html">DoubleConsumer</a> classes.</p>
<p>With the three functions bundled into one logical protocol or object it is easy then
to create complex or aggregate yet efficient parallelized reductions:</p>
<pre><code class="language-clojure">user&gt; (require '[ham-fisted.api :as hamf])
nil
user&gt; (hamf/preduce-reducers {:sum + :product *} (range 1 20))
{:product 121645100408832000, :sum 190}
user&gt;
</code></pre>
<h2>Consumers, Transducers, and <code>rfn</code> Chains</h2>
<p>If we look at the reduction in terms of a push model as opposed to a pull model where the
stream will push data into a consumer then we can implement similar map,filter chains
that are based around create a new consumer that takes the older consumer and the predicate
or mapping function.  In this way we can both implement a pipeline on the input stream
and we can implement perhaps diverging pipelines on each reduction function in a
multiple reducer scenario.  Since our init and merge functions operate accumulator
space then remains unchanged so we can build up more and more sophisticated reduction
functions and then still perform a parallelized reduction.  We then build up things in reverse
which is the reason that <code>comp</code> works in reverse when working with transducers.</p>
<p>In fact, given that we now know the game about composing reduction functions, the definition
of the single argument <code>clojure.core/filter</code> I think is more clear:</p>
<pre><code class="language-clojure">(defn filter
  "Returns a lazy sequence of the items in coll for which
  (pred item) returns logical true. pred must be free of side-effects.
  Returns a transducer when no collection is provided."
  {:added "1.0"
   :static true}
  ([pred]
    (fn [rf]
      (fn
        ([] (rf))
        ([result] (rf result))
        ([result input]
           (if (pred input)
             (rf result input)
             result)))))))
</code></pre>
<p>It returns a function that, when given a reduction function, returns a new reduction
function that when called in the two argument form is identical to the result above
(although expressed in pure Clojure as opposed to Java).</p>
<p>If we start with the concept that a reduction starts at the collection, flows
downward through the pipeline and bottoms out at the reducer then the
lazy-noncaching namespace and java streams implement parallelization flowing from
the container downward while consumer chains and transducers implement the pipeline
flowing up from the reducer itself.  Thus we can build the pipeline either downward
from the source or upward from the final reduction and we get slightly different properties
but regardless one trait we would like to ensure correctness is to disable parallelization
where it will cause an incorrect answer - such as in a stateful transducer.</p>
<p>Broadly speaking, however, it can be faster to enable full parallelization and
filter invalid results than it is to force an early serialization our problem and
thus lose lots of our parallelization potential.  If we are concerned with
performance we should attempt to move our transformations as much as possible into a
parallelizable domain.</p>
<p>For the <code>take-n</code> use case specifically mentioned above and potentially for others we
can parallelize the reduction and do the take-n both in the parallelized phase and
in the merge phase assuming we are using an ordered parallelization, so that doesn't
itself necessarily force a serialized reduction but there are of course
transformations and reductions that do.  There are intermediate points however that are
perhaps somewhat wasteful in terms of cpu load but do allow for more parallelization - a
tradeoff that is sometimes worth it. Generically speaking we can visualize this sort
of a tradeoff as triangle of three points where one point is data locality, one
point parallelism, and one point redundancy.  Specifically if we are willing to
trade some cpu efficiency for some redundancy, for instance, then we often get more
parallelization.  Likewise if we are willing to save/load data from 'far' away from
the CPU, then we can cut down on redundancy but at the cost of locality.  For more
on this line of thinking please take a moment and read at least some of Jonathan Ragan-Kelly's
<a href="http://people.csail.mit.edu/jrk/jrkthesis.pdf">excellent PhD thesis</a> - a better explanation
of the above line of reasoning begins on page 20.</p>
<h2>Primitive Typed Serial Reductions</h2>
<p>This comes last for a good reason :-) - it doesn't make a huge difference in performance
but it should be noted allowing objects to implemented typed reductions:</p>
<pre><code class="language-java">default Object doubleReduction(IFn.ODO op, Object init);
default Object longReduction(IFn.OLO op, Object init)
</code></pre>
<p>where the next incoming value is a primitive object but the accumulator is still a generic
object allows us to use things like <code>DoubleConsumers</code> and <code>LongConsumers</code> and avoid boxing our
stream.  Furthermore if the aforementioned map and filter primitives are careful about
their rfn composition we can maintained a completely primitive typed pipeline through our
entire processing chain.</p>
<h2>One Final Note About Performance</h2>
<p>Collapsing reductions brings the source iteration pathway closer to the final
reduction pathway in terms of machine stack space which I believe allows HotSpot to
optimize the entire chain more readily.  Regardless of how good HotSpot gets,
however, parallelizing will nearly always result in a larger win but both work
together to enable peak performance on the JVM given arbitrary partially typed compositions
of sequences and reductions.</p>
<p>If we increase the data size yet again then we can of course use the same design to
distribute the computations to different machines.  As some people have figured out, however,
simply implementing the transformations you need efficiently reduces or completely eliminates
the need to distribute computation in the first place leading to a simpler, easier to test and
more robust system.</p>
<p>Ideally we can make achieving great performance for various algorithms clear and easy and
thus avoid myriad of issues regarding distributing computing in the first place.</p>
<ul>
<li>The first rule of distributed systems is to avoid distributing your computation in the first place - <a href="https://bravenewgeek.com/service-disoriented-architecture/">1</a>, <a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">2</a>.</li>
<li>The first law of distributed objects is to avoid using distributed objects. <a href="https://martinfowler.com/bliki/FirstLaw.html">3</a>.</li>
</ul>
</div></div></div></body></html>